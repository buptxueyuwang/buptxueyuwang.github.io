---
title: KAN网络
tags:
  - Transformer
top: 
category: 学习
abbrlink: kan_model
---
---

# AI大讲堂：深度学习要变天？专业拆解【KAN网络】

![image-20240620161210708](https://s2.loli.net/2024/06/20/Nz5EFfrxpJPMbgy.png)

人话讲论文系列：KAN模型（上）.pdf

MLP本质回顾

$f(x)=wx+b$



## KAN网络为什么牛逼？



Kolmogorov-Arnold Networks顾名思义基于柯尔莫果洛夫-阿诺尔德表示定理。是由这两个俄罗斯数学家1957年提出的如何用一组较简单的函数来表示任何一个多变量的连续函数。

$f(\mathbf{x})=\sum_{q=1}^{2n+1}\Phi_q\left(\sum_{p=1}^n\phi_{q,p}(x_p)\right)$

想象一下，你有一个非常复杂的配方，需要各种各样的原料和步骤来制作一道菜。柯尔莫果洛夫阿诺尔德表示定理告诉我们，无论这个配方多么复杂，我们总能找到一种方法，通过一些简单的基本步骤（这里是一些基本的函数）来重现这道菜的味道。

在上面的式子中，输入是×，$\phi q$,$p(xp)$是基本的一元函数，就像是青椒西红柿基本原料的处理。内层求和就是放到一起。$\phi q$是外层的函数，各自接受内层求和的结果作为输入。外层的求和∑表示整个函数灯是子函数中q的和。用图来表示就相当于一个两层的神经网络，区别在于一没了线性组合，而是直接对输入进行激活：二来这些激活函数不是固定的，而是可以学习的。

![image-20240620163842529](https://s2.loli.net/2024/06/20/nJIUH6pt3bCAkDZ.png)

1. learnable activation functions on deges
2. sum opearation on nodes

和MLP每层统一进行非线性空间变换相比，这相当于对每个坐标轴单独进行非线性变换，然后再组合形成多维度空间。（画个简图，先组合再变形和先单个变形再简单组
合的区别）

公式写成向量的形式就是

$\mathrm{KAN}(\mathbf{x})=(\mathbf{\Phi}_{3}\circ\mathbf{\Phi}_{2}\circ\mathbf{\Phi}_{1})(\mathbf{x})$

$\mathrm{MLP}(\mathbf{x})=(\mathbf{W}_{3}\circ\sigma_{2}\circ\mathbf{W}_{2}\circ\sigma_{1}\circ\mathbf{W}_{1})(\mathbf{x})$

对比MLP，没有了激活函数和参数矩阵的嵌套关系，而直接是非线性函数φ的嵌套。
对于多层网络，这相当于下面的结构：

![image-20240620164411364](https://s2.loli.net/2024/06/20/M15Itl2zyxoS3ua.png)

注意这里所有的非线性函数Φ都采用同样的函数结构，只是用不同的参数来控制其形状。具体
来说，文章选择了数值分析中的样条函数`spline`。这个英语单词spline来源于可变形的样条
工具，那是一种在造船和工程制图时用来画出光滑形状的工具。

![image-20240620164659721](https://s2.loli.net/2024/06/20/s5JUluEXC48aP1e.png)

样条函数的研究始于20世纪中叶，到了60年代它与计算机铺助设计相结合，在外形设计方面得到成功的应用。样条理论已成为函数逼近的有力工具。



对比MLP和KAN,最大的区别就是变固定的非线性激活+线性参数学习为直接对参数化的非线性激活函数的学习。因为参数本身的复杂度，显然单个spie函数的学习难度要比线性函数难，但KANs通常允许比MLPs更小的计算图，也就是实现同样效果，需要的网络规模更小。例如，文章展示了在解偏微分方程(PDE)的过程中，一个2层宽度为10的KAN比一个4层宽度为100的MLP具有更高的准确度（均方误差10^-7对比10^-5)并且具有更高的参数效率(参数数量100对比10000)。



到这里为止，你一定好奇，这DEA不复杂啊，难道以前没人想到，有，但是卡壳在都坚持使用原始的二层宽度为(2+1)的表示方法，并没有机会利用更现代的技术（例如，反向传播）来训川练网络。KAN模型的贡献就在于通过进一步简化推广到任意宽度和深度，同时通过广泛的实证实验论证了在A!+科学方面的效果，而且具备很好的准确性和可解释性。这就牛逼了啊，深度学习最大的问题就是个黑盒子，训练网络像是炼丹。大模型越弄越大，很可能一条道走到黑就进死胡同了。好比芯片的摩尔定律。现在出现了量子芯片，原理上就不同，从而有可能实现根本性的变革。当然，原来的各种网络结构还能平替重做一遍，有没有感觉一片Al新大陆向你招手了。我一直劝大家别太短视，成天只盯着transformer,大模型兜兜转转，撑死了也是井中之蛙。



好，咱们接着走进技术细节，来把KAN网络扒光。

![image-20240620165132329](https://s2.loli.net/2024/06/20/7vwgAEYTnoNkxsK.png)

整篇文章用了这样一张图来装逼，大部分都是废话，关键词就是仁：`数学上有据可依`，用实
验分别证明了`准确性`和`可解释性`。

## 四、KAN的架构细节

## 4.1详细解释

整个网络架构原理看图一目了然。很多个这种类以四分之三个周期的正弦函数组合起来就能拟
合任意形状的函数。换句话说，用B-spline这一种激活函数两次求和就够了。

![image-20240620165816809](https://s2.loli.net/2024/06/20/hTgqElxaGNvCJAS.png)

图中展示的结构中，使用了两种尺度或分辩率的组合：粗粒度和细粒度网格，在保持
计算效率的同时，更加精确地捕捉和适应函数的变化。这种基础结构其实并不是很难
想到，以前就有了，但难点是怎么把它变深，否则单靠这么点玩意儿是不能逼近复杂
函数的。这就是本文的主要贡献了。

要构建深层KAN,首先需要回答：“什么是KAN层？”简单说，它就是一个一维函数矩
阵。

$\Phi=\{\phi_{q,p}\},\quad p=1,2,\cdots,n_{\mathrm{in}},\quad q=1,2\cdots,n_{\mathrm{out}},$

这样以来，前面的两层KAN网络第一层输入有n个，输出为2n+1个，第二层输入为
2+1,最终输出为1个。想更深就简单堆叠好了，和MLP一个球样。简单说就是找
层输入输出之间的转移矩阵

$\mathbf{x}_{l+1}=\underbrace{\begin{pmatrix}\phi_{l,1,1}(\cdot)&\phi_{l,1,2}(\cdot)&\cdots&\phi_{l,1,n_l}(\cdot)\\\phi_{l,2,1}(\cdot)&\phi_{l,2,2}(\cdot)&\cdots&\phi_{l,2,n_l}(\cdot)\\\vdots&\vdots&&\vdots\\\phi_{l,n_{l+1},1}(\cdot)&\phi_{l,n_{l+1},2}(\cdot)&\cdots&\phi_{l,n_{l+1},n_l}(\cdot)\end{pmatrix}}_{\Phi_l}\mathbf{x}_l,$



这里`l`是层编号，右边为输入，左边为输出。看上面左图就大致明白对应关系，输入为2个，因此第二层是2×2+1=5个。，订就是每条边上的激活函数，也就是非线性变
换。相当于每个×都有5个分身，然后再分别组合。其中i用来标记当前层的节点，而j用来标记下一层的节点。每个节点x_,i的输出通过激活函数中_，，j处理后，贡献到所有下一层的x+1,的计算中。对应上面左图，输入层2个节点，第二层5个节点，因此矩阵为5*2。矩阵的第一列表示×0,1对应的5个激活函数，第二列对应X0,2的，然后两两组合。

因此，这里需要强调的是KAN网络层节点数不是随便搞的，由输入节点个数确定2n+1个，然后所需要的参数或者连接数为(2n+1)*n,明显比全连接少了不少，看图就知道。

进而把多层函数级联关系写成矩阵形式就是:

$\mathrm{KAN}(\mathbf{x})=(\boldsymbol{\Phi}_{L-1}\circ\boldsymbol{\Phi}_{L-2}\circ\cdots\circ\boldsymbol{\Phi}_1\circ\boldsymbol{\Phi}_0)\mathbf{x}.\quad(2.7)$

或者展开的形式

$f(\mathbf{x})=\sum_{i_{L-1}=1}^{n_{L-1}}\phi_{L-1,i_{L},i_{L-1}}\left(\sum_{i_{L-2}=1}^{n_{L-2}}\cdots\left(\sum_{i_{2}=1}^{n_{2}}\phi_{2,i_{3},i_{2}}\left(\sum_{i_{1}=1}^{n_{1}}\phi_{1,i_{2},i_{1}}\left(\sum_{i_{0}=1}^{n_{0}}\phi_{0,i_{1},i_{0}}(x_{i_{0}})\right)\right)\right)\cdots\right),$

再小结一下，原始的两层KAN网络的形状是`[n,2n+1,1]`,现在变成了`多层级联`。这里貌似进一步又放开了2+1的结构限制，甚至隐层节点数可以自由发挥。

![image-20240620170705266](https://s2.loli.net/2024/06/20/qiENmtzk954Mr76.png)

比如这个图中显然隐层个数不是2+1，但依然是各自对应位置的求和，而不是全连
接。
`执行细节`
1.`残差激活函数`：我们包含了一个基础函数b(x)（类似于残差连接中的函数），使
得激活函数(x)是基础函数b(x)和样条函数的总和：

$\phi(x)=w(b(x)+\mathrm{spline}(x)).$

我们通常设置

$b(x)=\text{silu}(x)=\frac x{1+e^{-x}}$

样条函数通常参数化为B样条的线性组合：

$\mathrm{spline}(x)=\sum_ic_iB_i(x)$

其中c:是可训练的。原则上w是多余的，因为它可以被吸收进b(x)和spline(c).
然而，我们仍然包括这个w因子以更好地控制激活函数的整体大小。

SiLU(Sigmoid Linear Unit)是一种神经网络激活函数，也被称为Swish函数。这个函数由一篇Google Brain的论文首次提出，并因其在某些任务上表现出的优异性能而受到关注。你可以认为它就是sigmoid函数的一种变体。



2.假设层宽相等，L层，每层N个节点。
2.每个样条函数的阶数通常为k=3,在G个区间上G+1个网格点。“G个区间”指的是样条函数的分段定义的区间数。

那么总共大约有O(N2L(G+k)或O(N2LG)个参数。相比之下，具有深度L和宽度N的多层感知机(MLP)只需要O(N2L)个参数，这看起来比KAN更有效率。

也就是说单看计算复杂度好像KAN还不如MLP简单，但是幸运的是，KANs通常需要
比MLPs小得多的NN,这不仅节省了参数，而且还提高了泛化能力，并且有助于解
释性。
换句话理解，就是借助spline样条函数的表达能力，无需很多节点就能实现比较强的
表达能力，因此总的来说，可以比MLP节省不少参数量。

## 4.2逼近能力和缩放定律的讨论

文章花了一页的篇幅推导证明了定理

$\begin{aligned}&\textbf{Theorem 2.1 (Approximation theory, KAT). Let x}=(x_{1},x_{2},\cdots,x_{n}).\textit{Suppose that a function}\\&f(\mathbf{x})admitsarepresentation\end{aligned}$

$f=(\Phi_{L-1}\circ\Phi_{L-2}\circ\cdots\circ\Phi_{1}\circ\Phi_{0})\mathbf{x},\quad(2.14)$

as in Eq.(2.7),where each one of the ii are (k+1)-times continuously differentiable.Then
there exists a constant C depending on f and its representation,such that we have the following
approximation bound in terms of the grid size G:there exist k-th order B-spline functions
such that for any0≤m≤k,we have the bound

$\|f-(\Phi_{L-1}^G\circ\Phi_{L-2}^G\circ\cdots\circ\Phi_1^G\circ\Phi_0^G)\mathbf{x}\|_{C^m}\leq CG^{-k-1+m}.\quad(2.15)$

这部分讲的不是人能听懂的话，看不懂很正常。简单说，就是从数学上证明可以通过构建多层的B样条函数网络来有效逼近复杂函数。尽管增加网络的深度和复杂度，KANs能够通过细致的网格划分来逼近高维函数，而不会受到维数灾难的影响，也就是在高维空间中，数据的稀疏性和处理复杂度急剧增加的问题。而残差率不依赖于维度，因此战胜了维数灾难！

再来看看所谓的缩放定律。注意这里的缩放定律与大模型领域的不同。后者是说模型
大小（如参数数量）的增加，模型的性能（例如在语言任务中的准确性）通常会提
高，并且有时这种提升的速度可以用某些数学关系（如幂律关系）来描述C=6ND。这
里更偏重于理论和数学上的分析，当然背景相似，都是讨论随着参数数量的增加，模
型表现的提升。这部分内容基本上也可以暂时略过，主要就是简要对比了几种理论，
关注于如何通过理论来指导实际的神经网络设计，以实现更有效的学习和泛化能力。
后面还有讨论，这里暂时可以忽略。
好，我们接下来重点看看KAN准确性和可解释性的改进。

## 4.3如何提升准确性？

`MLPs`通过增加模型的宽度和深度可以提高性能，但这种方法效率低下，因为需要独立地训练不同大小的模型。

`KANs`:开始可以用较少的参数训川练，然后通过简单地细化其
样条网格来增加参数，无需重新训练整个模型。
基本原理就是通过将样条函数(splines)I旧的粗网格转换为更细的网格，并对应地调
整参数，无需从头开始训练就能扩展现有的KAN模型。这种技术称为“网格扩展”
(grid extension)

$\text{Fitting }f(x,y)=\exp(\sin(\pi x)+y^2)$

![image-20240620172527644](https://s2.loli.net/2024/06/20/RorngTDAOxVIh8d.png)

文章用了一个小例子来证明这一点。用KAN网络逼近一个函数。上图中横轴的每个"gid-x"标签代表了在特定训练步骤时进行网格细化的时点。每次这样的标记出现，都意味着网格点数量在这个步骤有所增加，从而使模型能够更细致地逼近目标函数，这通常会导致误差的下降。表明网格点的增加直接影响了模型的学习效果，提高了逼近目标函数的精度。左右图表示了两种不同结构的网络。



下面两个图分别展示了测试误差随网格大小变化（左下图）和训练时间随网格大小的变化（右下图）。结论就是误差loss随网格大小grid size G在不同的规模上显示出不同的缩放关系；训练时间随网格大小增加而增长，特别是在网格非常大时（接近1000),训练时间急剧上升。
这些观测结果支持了文章中关于KANs利用网格扩展可以有效提高精度而无需重新训练整个模型的说法，同时也提示了在选择网格大小时可能需要在模型精度和训练效
率之间做出权衡。简单说，网格太密了也不好，太费时。

## 4.4如何提升可解释性？

尽管上面介绍了KN的不少好处，但遇到实际问题时该怎么设计网络结构依然是个玄学。因此需要有种方法能自动发现这种结构。本文提出的方法是使用稀疏正则化和剪枝技术从较大的KAN开始训练，剪枝后的KAN比未剪枝的KAN更易解释。为了使KAN达到最大的可解释性，本文提出了几种简化技术，并提供了一个示例，说明用户如何与KAN进行交互以增强可解释性。

![Figure 2.4:An example of how to do symbolic regression with KAN.](https://s2.loli.net/2024/06/20/KkWZLdjAPtIqSpM.png)

1.稀疏化：使用数据集训练一个KAN模型，使其能够尽可能地拟合目标函数。MLP通常使用L1正则化来促进权重的稀疏性，L1正则化倾向于推动权重值向零收缩，特别是那些对模型输出影响不大的权重。权重矩阵的“稀疏化“可以降低模型的复杂性，减少模型的存储需求和计算负担，因为只需要处理非零权重；还能提高模型的泛化能力，减少过拟合的风险
2.剪枝：在稀疏化后，进一步通过剪枝技术移除那些不重要的连接和神经元。
3.设定特定激活函数：根据剪枝后各神经元的特性，手动设置或调整特定神经元的激活函数

 4.训练仿射参数：在调整了激活函数后，对模型中的剩余参数进行再次训练，优化这
些参数以最好地拟合数据。
5.符号化：最终，模型将输出一个符号公式，这个公式是对原始目标函数的一个近似
表示，但通常会更简洁、更易于理解和分析。

## 五、实验论证

文章花了很大篇幅汇报详细的模拟实验结果，这也是本文引起轰动的重要原因之一。主要包括
准确性和可解释两部分。

### 5.1KAN的准确性

![image-20240620173526320](https://s2.loli.net/2024/06/20/sSwV2BfGxU6EdpJ.png)

比较了KAN与MLP在逼近5个典型函数上的性能，横轴是参数量，纵轴为均方根误差(RMSE)。总的来说，KAN和MLP随着参数数量的增加，RMSE都在下降。

在大多数情况下，KAN（蓝色线）比相同深度的MLP具有更低的RMSE,尤其是在参数数量较少时。这表明KANs在参数利用效率上可能更高。MLP在参数数量增加后，
性能提升逐渐放缓并迅速达到平台期，这可能是因为MLP对于这些类型的函数拟合存在固有的限制。KAN在多个测试案例中都接近或跟随理论曲线。

这表明KANs在处理复杂函数和高维数据时可能是更优的选择，具有更好的扩展性和效率。这种性能优势特别重要，当我们需要从有限的数据中学习复杂的模式时，如在
物理建模、声音处理或图像处理等任务中。当然目前还都是比较理论化的实验数据。

接着对比了KAN和MLP在高难度的特殊函数拟合任务上的性能，结论类似。随参数量增多KAN（蓝色）表现稳定，越来越好，而MLP（黄色）出现平台期。KANs在维持低误差的同时，表现出更好的参数效率和泛化能力。这一点对于设计高效且精确的机器学习模型来说是极其重要的，特别是在资源受限或对精度要求极高的应用中。

![image-20240620173830055](https://s2.loli.net/2024/06/20/dFPt1Vrz29T4s3G.png)

这些函数涉及椭圆积分和贝塞尔函数的特殊数学函数。

然后又提供了在解决偏微分方程上的例子。对比MLP,KANs在相同参数数量下实现
了更低的误差

![image-20240620173956201](https://s2.loli.net/2024/06/20/ux8ymLRZTwblgP3.png)

接着，讨论了连续学习问题，顶行展示了用于回归任务的一维数据，包含五个高斯峰数据按阶段顺序呈现，每个阶段只展示一部分数据峰。中间和底部的行分别展示了KAN和MLP在五个学习阶段的拟合结果。KANs能够在新增数据的学习中保持之前学到的知识，而MLP表现出严重的`灾难性遗忘`，即新学的信息`严重干扰了旧知识的记忆`。

![image-20240620174047112](https://s2.loli.net/2024/06/20/pqwW3xzu5thG6oN.png)

这种能力在机器学习中显然很重要。

## 5.2KAN的可解释性

### 5.2.1蓝督学习的例子

借助前面提升模型可解释性的小技巧，包括稀疏化、剪枝等，KAN网络最终形成的网络结构
不仅能够实现数学函数的拟合，而且其形式本身能反映出被拟合函数的内在结构。

![Figure 4.1:KANs are interepretable for simple symbolic tasks](https://s2.loli.net/2024/06/20/9lJRSaifOWwEPCH.png)

以第一个图为例
函数：f(c,y)=xy
解释：
图中的结构利用了恒等式$2xy=(x+y)^2-x^2-y^2$来计算乘法。这说明
KAN通过结合基本运算(加法、平方)来实现复杂的乘法操作，展示了KAN如何通
过基本的数学操作构造更复杂的函数。
X和y各自经过线性函数求和，然后平方，同时再减去X和y的平方。

因此可以看出，KAN模型的牛逼之处在于两点：首先，不仅仅在于自身的模型结构，MLP是
先组合再非线性激活，KAN是先非线性激活再组合：其次，KAN的训川练能实现自身结构上的
优化，有点自组织的味道了。

#### 5.2.2非监督学习的例子

上面是个监督学习的例子，用来逼近合成数据。再来看一个非监督学习的例子。

![Figure 4.2:Unsupervised learning of a toy task.KANs can identify groups of dependent variables,i.e..
(1,2,3)and (4,6)in this case.](https://s2.loli.net/2024/06/20/J6XR4tsPvdw2DOH.png)

在无监督学习中，目标是识别数据中变量之间的依赖关系，而不是预测输出。KANs通过修改其结构，能够识别哪些输入变量是相互依赖的。左图(seed-0)和右图(seed=2024)显示了相同的数据集但不同的初始化种子如何导致KAN学到不同的依赖关系结构。
KAN通过其灵活的网络结构提供了一种强大的工具来探索这些关系，从而增强了摸型的解释性和应用的广泛性。



#### 5.2.3数学领域的应用示例

用KAN来处理结点理论问题。结点理论(Knot Theory)是拓扑学的一个分支，专门研
究数学中的结。

图ā显示使用17个变量的网络结构实现了81.6%的测试准确率。仅使用3个最重要的变量精简后的模型达到了78.2%的测试准确率。图(c)通过饼图展示了三个变量对预测结
果的贡献比例。

![Figure 4.3:Knot dataset,supervised mode.With KANs,we rediscover Deepmind's results that signature is
mainly dependent on meridinal translation(real and imaginary parts).](https://s2.loli.net/2024/06/20/j6JQtLf9F4AklCD.png)

通过优化输入变量的选择，KAN能够在保持较高准确性的同时，显著减少模型复杂
度。这一点对于希望减少计算资源消耗同时提高模型解释性的应用场景尤为重要。
也就是说，KAN的训练算法能够某种程度上实现网络结构的自我选择和优化。
如果说上面的例子还是监督学习，只能用于验证，那么利用无监督学习KAN还能用来发现新的数据结构，这就是是下面的例子。

![Figure 4.4:Knot dataset,unsupervised mode.With KANs,we rediscover three mathematical relations in the
knot dataset.](https://s2.loli.net/2024/06/20/UjNoA9zTrCl7Oyk.png)

图a重新发现签名依赖性；图(b)展示了KAN如何在不需要任何先验知识的情况下，通过自学习过程重新发现尖点体积定义。图c)中数据点围绕着直线y=x/2分布，表明y和x之间存在直接的线性关系。y始终小于或等于x的一半，相当于重新发现一个不等式。这种自发现的能力说明了KAN可以作为一种探索数据中隐藏模式的强大工具。

####  5.2.4物理领域的应用示例

接着，本文继续用KAN来探索和解释物理模型中的动力学边界，尤其是在量子系统的安德森局域化现象中的应用。

![Figure 4.5:Results for the Mosaic Model.Top:phase diagram.Middle and Bottom:KANs can obtain both
qualitative intuition (bottom)and extract quantitative results(middle).5is the golden ratio.](https://s2.loli.net/2024/06/20/elUFm2J63df8iLt.png)

这个图展示了把KAN应用于两个具体的物理模型。
顶部：显示了模型的相图，描绘了系统参数变化下的物理状态，具体是啥咱也不用懂。
中部：展示了系统的特征尺寸随系统参数变化的行为，这有助于量化电子状态的局部化程度。具体是啥也不用懂。
底部：提供了对应的KAN结构，展示了网络如何通过学习输入数据（系统参数）来输出对应的物理行为（如局部化状态），并突出了关键的网络节点和连接，这有助于
理解模型中最重要的物理量。反正就是说结构能有助于理解。
那既然KAN训练本身能自动选择结构，而结构又能有一定物理意义。那这种结构优化能人为控制吗？于是文章又举了一个这样的例子：自动和手动模式的比较。

![Figure 4.6:Human-KAN collaboration to discover mobility edges of GAAM and MAAM.The human user can
choose to be lazy (using the auto mode)or more involved (using the manual mode).More details in text.](https://s2.loli.net/2024/06/20/jXifSbVEyF2to7c.png)

自动模式下，KANs完全基于数据自动构建和优化网络结构，无需人工干预。这可以
快速得到一个工作模型，但可能缺乏针对复杂问题的深入定制。在手动模式下，用户
可以更深入地介入模型的构建过程，例如通过手动设置网络层和激活函数来探索数据
中的特定特征或关系。这种方法虽然更耗时，但可以更精确地调整模型以适应特定的
科学问题。通过结合KANs的自动化能力和用户的专业知识，可以有效地发掘和解释
物理系统中的关键现象，尤其是在处理难以用传统方法解析的复杂系统时。

## 六、MLP和KAN到底该怎么选？

![Figure 6.1:Should I use KANs or MLPs?](https://s2.loli.net/2024/06/20/ARo68nEGHqmKXPI.png)

关于这个问题，主要看想要的是什么？如果效率优先，也就是最右边这条支路，选MLP,因
为目前，KANs训练速度较慢是其主要瓶颈，通常比MLPs慢10倍。但如果想要小模
型，KAN更好。如果可解释性优先，选中间，那么KAN牛逼。如果准确性优先，最左边，KAN也更牛逼。
尽管KAN显示了不错的前景，但毕竟刚开始，还很`不足`。这包括：
`数学方面`，其实对Kolmogorov-Arnold表示定理做了很多简化，而且这个定理本身并没有考虑深层情况，也许增加深度概念后数学基础更强。
`算法方面`，还没有充分探索KANs的架构设计和训练方法，以改进模型准确性。比如
替换spline激活函数，也许更好。效率方面还有待提升。同时，是不是可以与MLP混合使用也值得考虑。

七、代码实现
来到Github上看代码，文件夹包括了论文和代码。后者在kan文件夹。
https://github.com/KindXiaoming/pykan

打开代码文件夹，目录结构很简洁。
KAN.py是主要的文件，包含了定义KAN模型的主要类或函数，负责模型的建立和训
练流程。
KANLayer..py定义了KAN模型中使用的自定义层，包括模型的核心组件，如特殊的激
活层或其他处理层，这些是构成KAN的基础。
L-BFGS.py包含了适用于KANs训练的L-BFGS优化器的实现。
Symbolic_.KANLayer..py实现了一种特殊的KAN层，用于处理符号计算或增强模型的解
释性。将数据或激活函数转换为符号形式，以便进行更深入的分析或解释。
spline.py包含实现样条函数的代码

我们着重看下KAN.py和KANLayer.py两个文件。这个KAN.py文件定义了一个KAN类，继承自torch.nn.Module,主要用于实现一个基于卷积的神经网络模型。

`属性(Attributes)`
·biases:使用nn.Linear（）'定义的偏置列表。
·act_fun:使用KANLayer'定义的激活函数层列表。
·depth:网络的深度。
·width:每层的神经元数量。
grid:网格间隔数。
·k:分段多项式的阶数。
·base.fun:基函数，用于定义激活函数的基础形式
·symbolic..fun:使用Symbolic_.KANLayer定义的符号激活函数层列表。
symbolic..enabled:标志位，用于控制是否计算符号前端以节省时间。
`方法(Methods)`

iitO:初始化KAN模型，可以设定网络宽度、网格数、多项式阶数、噪声比例、基
函数等。
·forward0:前向传播函数，根据输入数据通过网络计算输出。
·train（）:训练模型，包括设定优化器、损失函数、正则化方法等。
prue(0:对模型进行剪枝，移除不重要的连接或节点以简化模型和减少计算量。
pot0:绘制网络结构图，显示网络各层连接和激活函数的详细信息。
symbolic.formula0:提取和返回网络的符号表达式，用于分析网络结构的数学形式。

`代码逻辑流程`
1.初始化：在类的构造函数中初始化网络结构，包括层的设置、氵
激活函数的初始化
等。
2.前向传播：定义前向传播逻辑，
如何通过每一层和每个激活函数计算输出。
3.训练过程：
设置训练参数，执行训川练循环，包括前向传播、损失计算、反向传播和
参数更新。
4.剪枝和可视化：提供方法对网络进行剪枝并可视化网络结构，以便分析和优化。
5.符号表达式提取：提取网络的数学符号表达式，用于深入理解网络的工作原理。
再来看看`KANLayer.py`文件，

## 八、小结

1.`MLP的硬伤`：我们回归了MLP的核心原理，线性组合+非线性激活。深层次化网络后，反
向传播求导数时单一激活函数的连乘积会产生很多问题，而且全连接网络导致参数效率低
下。
2.`KAN的原理`：用单一架构的参数化可学习非线性激活函数直接组合，实现非线性空间变
换。模型表征能力大大提升。
3.`KAN训练算法`：通过grid extension,也就是激活函数分辩率提升，以及稀疏化、剪枝等
结构自优化技巧，实现了准确性和可解释性的提升。能够在参数量大大减少的情况下实现
相同甚至更有的拟合效果。
4.`实验验证`：仿真实验提供了有效的量化的效果证明，展示了非常有前景的方向，但目前显
然还比较初级。不过，提供了一条新的道路。

俗话说，天下大事，分久必合，合久必分。深度学习的理论也已经十几年一成不变了。这就相当于建筑学上的砖混结构，曾经称霸数十年。当出现钢筋混凝土浇筑结构的时候，整个建造的方法就会发生翻天覆地的变化。KAN网络无疑展示出了能够替换传统的多层感知机结构的强大实力。也让众多的研究者看到了深度学习乃至于整个人工智能新的一大片蓝海。这也是为什
么能够让大家像打了鸡血一样兴奋起来的原因。以前不知道idea,现在借助这个结构可以重做一遍。
如果今天的分享对你有所帮助，欢迎三连支持。我是梗直哥。学好A!不迷路。只说人话，专治好奇。

```python
conda create -n kan620 python=3.9
conda activate kan620

conda install pytorch==1.10.0 torchvision==0.11.0 torchaudio==0.10.0 cudatoolkit=11.3 -c pytorch -c conda-forge
# 其他配套安装


pip install torchtext==0.11.0
pip install spacy
pip install pyitcast
pip install pandas
pip install matplotlib
pip install AEML
pip install tensorboard
pip install distutils

pip uninstall setuptools
pip install setuptools==59.5.0 //需要比你之前的低 
pip install scikit-learn
pip install seaborn


禁用 conda env remove --name <env_name>
pip install -r H:\PyTorch\4StockMarketPrediction\requirements.txt\requirement.txt

```

